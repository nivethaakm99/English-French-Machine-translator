{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('fra.txt', encoding='utf-8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sent = []\n",
    "fra_sent = []\n",
    "eng_chars = set()\n",
    "fra_chars = set()\n",
    "nb_samples = 10000\n",
    "\n",
    "# Process english and french sentences\n",
    "for line in range(nb_samples):\n",
    "    \n",
    "    eng_line = str(lines[line]).split('\\t')[0]\n",
    "    \n",
    "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
    "    fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
    "    eng_sent.append(eng_line)\n",
    "    fra_sent.append(fra_line)\n",
    "    \n",
    "    for ch in eng_line:\n",
    "        if (ch not in eng_chars):\n",
    "            eng_chars.add(ch)\n",
    "            \n",
    "    for ch in fra_line:\n",
    "        if (ch not in fra_chars):\n",
    "            fra_chars.add(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_chars = sorted(list(fra_chars))\n",
    "eng_chars = sorted(list(eng_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ' ', 1: '!', 2: '$', 3: '%', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '0', 10: '1', 11: '2', 12: '3', 13: '5', 14: '6', 15: '7', 16: '8', 17: '9', 18: ':', 19: '?', 20: 'A', 21: 'B', 22: 'C', 23: 'D', 24: 'E', 25: 'F', 26: 'G', 27: 'H', 28: 'I', 29: 'J', 30: 'K', 31: 'L', 32: 'M', 33: 'N', 34: 'O', 35: 'P', 36: 'Q', 37: 'R', 38: 'S', 39: 'T', 40: 'U', 41: 'V', 42: 'W', 43: 'Y', 44: 'a', 45: 'b', 46: 'c', 47: 'd', 48: 'e', 49: 'f', 50: 'g', 51: 'h', 52: 'i', 53: 'j', 54: 'k', 55: 'l', 56: 'm', 57: 'n', 58: 'o', 59: 'p', 60: 'q', 61: 'r', 62: 's', 63: 't', 64: 'u', 65: 'v', 66: 'w', 67: 'x', 68: 'y', 69: 'z'}\n",
      "{' ': 0, '!': 1, '$': 2, '%': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '0': 9, '1': 10, '2': 11, '3': 12, '5': 13, '6': 14, '7': 15, '8': 16, '9': 17, ':': 18, '?': 19, 'A': 20, 'B': 21, 'C': 22, 'D': 23, 'E': 24, 'F': 25, 'G': 26, 'H': 27, 'I': 28, 'J': 29, 'K': 30, 'L': 31, 'M': 32, 'N': 33, 'O': 34, 'P': 35, 'Q': 36, 'R': 37, 'S': 38, 'T': 39, 'U': 40, 'V': 41, 'W': 42, 'Y': 43, 'a': 44, 'b': 45, 'c': 46, 'd': 47, 'e': 48, 'f': 49, 'g': 50, 'h': 51, 'i': 52, 'j': 53, 'k': 54, 'l': 55, 'm': 56, 'n': 57, 'o': 58, 'p': 59, 'q': 60, 'r': 61, 's': 62, 't': 63, 'u': 64, 'v': 65, 'w': 66, 'x': 67, 'y': 68, 'z': 69}\n"
     ]
    }
   ],
   "source": [
    "# dictionary to index each english character - key is index and value is english character\n",
    "eng_index_to_char_dict = {}\n",
    "\n",
    "# dictionary to get english character given its index - key is english character and value is index\n",
    "eng_char_to_index_dict = {}\n",
    "\n",
    "for k, v in enumerate(eng_chars):\n",
    "    eng_index_to_char_dict[k] = v\n",
    "    eng_char_to_index_dict[v] = k\n",
    "\n",
    "print(eng_index_to_char_dict)\n",
    "print(eng_char_to_index_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\t', 1: '\\n', 2: ' ', 3: '!', 4: '$', 5: '%', 6: '&', 7: \"'\", 8: '(', 9: ')', 10: ',', 11: '-', 12: '.', 13: '0', 14: '1', 15: '2', 16: '3', 17: '5', 18: '8', 19: '9', 20: ':', 21: '?', 22: 'A', 23: 'B', 24: 'C', 25: 'D', 26: 'E', 27: 'F', 28: 'G', 29: 'H', 30: 'I', 31: 'J', 32: 'K', 33: 'L', 34: 'M', 35: 'N', 36: 'O', 37: 'P', 38: 'Q', 39: 'R', 40: 'S', 41: 'T', 42: 'U', 43: 'V', 44: 'Y', 45: 'a', 46: 'b', 47: 'c', 48: 'd', 49: 'e', 50: 'f', 51: 'g', 52: 'h', 53: 'i', 54: 'j', 55: 'k', 56: 'l', 57: 'm', 58: 'n', 59: 'o', 60: 'p', 61: 'q', 62: 'r', 63: 's', 64: 't', 65: 'u', 66: 'v', 67: 'x', 68: 'y', 69: 'z', 70: '\\xa0', 71: '«', 72: '»', 73: 'À', 74: 'Ç', 75: 'É', 76: 'Ê', 77: 'à', 78: 'â', 79: 'ç', 80: 'è', 81: 'é', 82: 'ê', 83: 'ë', 84: 'î', 85: 'ï', 86: 'ô', 87: 'ù', 88: 'û', 89: 'œ', 90: '\\u2009', 91: '’', 92: '\\u202f'}\n",
      "{'\\t': 0, '\\n': 1, ' ': 2, '!': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, '(': 8, ')': 9, ',': 10, '-': 11, '.': 12, '0': 13, '1': 14, '2': 15, '3': 16, '5': 17, '8': 18, '9': 19, ':': 20, '?': 21, 'A': 22, 'B': 23, 'C': 24, 'D': 25, 'E': 26, 'F': 27, 'G': 28, 'H': 29, 'I': 30, 'J': 31, 'K': 32, 'L': 33, 'M': 34, 'N': 35, 'O': 36, 'P': 37, 'Q': 38, 'R': 39, 'S': 40, 'T': 41, 'U': 42, 'V': 43, 'Y': 44, 'a': 45, 'b': 46, 'c': 47, 'd': 48, 'e': 49, 'f': 50, 'g': 51, 'h': 52, 'i': 53, 'j': 54, 'k': 55, 'l': 56, 'm': 57, 'n': 58, 'o': 59, 'p': 60, 'q': 61, 'r': 62, 's': 63, 't': 64, 'u': 65, 'v': 66, 'x': 67, 'y': 68, 'z': 69, '\\xa0': 70, '«': 71, '»': 72, 'À': 73, 'Ç': 74, 'É': 75, 'Ê': 76, 'à': 77, 'â': 78, 'ç': 79, 'è': 80, 'é': 81, 'ê': 82, 'ë': 83, 'î': 84, 'ï': 85, 'ô': 86, 'ù': 87, 'û': 88, 'œ': 89, '\\u2009': 90, '’': 91, '\\u202f': 92}\n"
     ]
    }
   ],
   "source": [
    "# dictionary to index each french character - key is index and value is french character\n",
    "fra_index_to_char_dict = {}\n",
    "\n",
    "# dictionary to get french character given its index - key is french character and value is index\n",
    "fra_char_to_index_dict = {}\n",
    "for k, v in enumerate(fra_chars):\n",
    "    fra_index_to_char_dict[k] = v\n",
    "    fra_char_to_index_dict[v] = k\n",
    "    \n",
    "print(fra_index_to_char_dict)\n",
    "print(fra_char_to_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
    "max_len_fra_sent = max([len(line) for line in fra_sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_eng_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_fra_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
    "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
    "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the english and french sentences\n",
    "\n",
    "for i in range(nb_samples):\n",
    "    for k,ch in enumerate(eng_sent[i]):\n",
    "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
    "        \n",
    "        \n",
    "    for k,ch in enumerate(fra_sent[i]):\n",
    "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
    "\n",
    "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "        if k > 0:\n",
    "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
    "print(tokenized_eng_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_7:0\", shape=(?, ?, 70), dtype=float32)\n",
      "\n",
      " [<tf.Tensor 'lstm_7_1/TensorArrayReadV3:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_7_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_7_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]\n",
      "\n",
      " Tensor(\"lstm_7/TensorArrayReadV3:0\", shape=(?, 256), dtype=float32)\n",
      "\n",
      " Tensor(\"lstm_7/while/Exit_2:0\", shape=(?, 256), dtype=float32)\n",
      "\n",
      " Tensor(\"lstm_7/while/Exit_3:0\", shape=(?, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Encoder model\n",
    "\n",
    "encoder_input = Input(shape=(None,len(eng_chars)))\n",
    "print(encoder_input)\n",
    "encoder_LSTM = LSTM(256,return_state = True)\n",
    "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
    "print('\\n',encoder_LSTM (encoder_input))\n",
    "print('\\n',encoder_outputs)\n",
    "print('\\n',encoder_h)\n",
    "print('\\n',encoder_c)\n",
    "encoder_states = [encoder_h, encoder_c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder model\n",
    "\n",
    "decoder_input = Input(shape=(None,len(fra_chars)))\n",
    "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
    "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
    "decoder_out = decoder_dense (decoder_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0816 22:48:28.132311 4521743808 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0816 22:48:28.176691 4521743808 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0816 22:48:28.339952 4521743808 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0816 22:48:29.952976 4521743808 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.9134 - val_loss: 0.9390\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.7208 - val_loss: 0.7545\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6164 - val_loss: 0.6921\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.5610 - val_loss: 0.6303\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.5198 - val_loss: 0.6017\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.4880 - val_loss: 0.5638\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.4622 - val_loss: 0.5380\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.4400 - val_loss: 0.5235\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.4203 - val_loss: 0.5100\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.4039 - val_loss: 0.4995\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3881 - val_loss: 0.4861\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3737 - val_loss: 0.4753\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3606 - val_loss: 0.4663\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3478 - val_loss: 0.4589\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3361 - val_loss: 0.4592\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3246 - val_loss: 0.4537\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3142 - val_loss: 0.4468\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.3040 - val_loss: 0.4502\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2941 - val_loss: 0.4463\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2852 - val_loss: 0.4410\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2761 - val_loss: 0.4485\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2679 - val_loss: 0.4505\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2598 - val_loss: 0.4465\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2519 - val_loss: 0.4479\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2441 - val_loss: 0.4526\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2372 - val_loss: 0.4595\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2301 - val_loss: 0.4473\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2236 - val_loss: 0.4522\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2169 - val_loss: 0.4560\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2109 - val_loss: 0.4644\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2049 - val_loss: 0.4634\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.1992 - val_loss: 0.4572\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.1938 - val_loss: 0.4651\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 42s 5ms/step - loss: 0.1887 - val_loss: 0.4684\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1835 - val_loss: 0.4691\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1787 - val_loss: 0.4746\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.1741 - val_loss: 0.4786\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.1693 - val_loss: 0.4824\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1652 - val_loss: 0.4901\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.1609 - val_loss: 0.4964\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.1566 - val_loss: 0.4978\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1531 - val_loss: 0.5007\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1493 - val_loss: 0.5070\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1458 - val_loss: 0.5073\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1423 - val_loss: 0.5178\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.1390 - val_loss: 0.5166\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.1356 - val_loss: 0.5158\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.1324 - val_loss: 0.5250\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1299 - val_loss: 0.5268\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1269 - val_loss: 0.5358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2be51fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
    "          y=target_data,\n",
    "          batch_size=64,\n",
    "          epochs=50,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference models for testing\n",
    "\n",
    "# Encoder inference model\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
    "                                                 initial_state=decoder_input_states)\n",
    "\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
    "                          outputs=[decoder_out] + decoder_states )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(inp_seq):\n",
    "    \n",
    "    # Initial states value is coming from the encoder \n",
    "    states_val = encoder_model_inf.predict(inp_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
    "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
    "    \n",
    "    translated_sent = ''\n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:\n",
    "        \n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "        \n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
    "        translated_sent += sampled_fra_char\n",
    "        \n",
    "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "        \n",
    "        states_val = [decoder_h, decoder_c]\n",
    "    return translated_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Poursuis !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Salut.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Qui ?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Attentez !\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Attendez Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Va !\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Saute.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
    "    translated_sent = decode_seq(inp_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', eng_sent[seq_index])\n",
    "    print('Decoded sentence:', translated_sent)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
